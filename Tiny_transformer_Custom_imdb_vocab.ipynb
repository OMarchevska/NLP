{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0eea586-304c-4021-85c9-d323b37aa47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.5 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# after installation and kernel upgrade restart needed\n",
    "!pip install -q --upgrade keras-nlp\n",
    "!pip install -q --upgrade keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b126eab-628c-4831-8d4a-c373f001d8ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "import keras_nlp\n",
    "import tensorflow as tf\n",
    "import shutil\n",
    "import keras\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6a954c9-9d69-468e-ad45-d5547a74ed6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "\n",
    "dataset = tf.keras.utils.get_file('aclImdb_v1.tar.gz', url,\n",
    "                                  untar=True, cache_dir='.',\n",
    "                                  cache_subdir='')\n",
    "\n",
    "dataset_dir = os.path.join(os.path.dirname(dataset), \"aclImdb\")\n",
    "\n",
    "# set training and testing data paths\n",
    "train_dir = os.path.join(dataset_dir, \"train\")\n",
    "test_dir = os.path.join(dataset_dir, \"test\")\n",
    "\n",
    "# remove unused folders to make it easier to load the data\n",
    "remove_dir = os.path.join(train_dir, 'unsup')\n",
    "shutil.rmtree(remove_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03e85619-e3f9-4ca4-b8a4-164a64105701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "\n",
      "Class names: ['neg', 'pos']\n"
     ]
    }
   ],
   "source": [
    "# create datasets\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "batch_size = 32\n",
    "seed = 42\n",
    "\n",
    "train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    train_dir, batch_size=batch_size, validation_split=0.2,\n",
    "    subset='training', seed=seed)\n",
    "\n",
    "val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    train_dir, batch_size=batch_size, validation_split=0.2,\n",
    "    subset='validation', seed=seed)\n",
    "\n",
    "test_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    test_dir, batch_size=batch_size)\n",
    "\n",
    "class_labels = train_ds.class_names\n",
    "print(\"\\nClass names:\", class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47ff1246-e662-4f9b-9488-7756621fda3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in both train and valid datasets: 121894\n",
      "Reviews contain 232 words on average, with standard deviation of 173.0606689453125 words\n",
      "Minimum/maximum review word count: 10/2469\n"
     ]
    }
   ],
   "source": [
    "# concat reviews texts from train and validation datasets discarding labels \n",
    "reviews = tf.concat(([review for review, _ in train_ds.unbatch()], \n",
    "                     [review for review, _ in val_ds.unbatch()]), axis=0)\n",
    "# TextVectorization layer allows efficiently extract vocabulary from the text \n",
    "# while optionally appling standardization to it\n",
    "vectorizer = tf.keras.layers.TextVectorization()\n",
    "vectorizer.adapt(reviews)\n",
    "# compute the number of tokens per each review\n",
    "bows = vectorizer(reviews)\n",
    "counts = tf.math.count_nonzero(bows, axis=1, keepdims=True)\n",
    "# get a mean and standard deviation of reviews word counts \n",
    "mean, std = tf.math.reduce_mean(counts), tf.math.reduce_std(tf.cast(counts, dtype=tf.float32))\n",
    "min, max = tf.reduce_min(counts), tf.reduce_max(counts)\n",
    "\n",
    "print(\"Number of unique words in both train and valid datasets:\", vectorizer.vocabulary_size())\n",
    "print(f\"Reviews contain {mean} words on average, with standard deviation of {std} words\")\n",
    "print(f\"Minimum/maximum review word count: {min}/{max}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a752ffb-cb3c-4fc8-bb63-ebf0f29e3492",
   "metadata": {},
   "source": [
    "## Custom vocabulary from the IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f89276e7-ea24-4017-addc-24b9bf2cbfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(vocab_size, dataset, **kwargs):\n",
    "    vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
    "        dataset.map(lambda x, y: x),\n",
    "        vocabulary_size=vocab_size,\n",
    "        lowercase=True,\n",
    "        strip_accents=True,\n",
    "        reserved_tokens=[\"[PAD]\", \"[START]\", \"[END]\", \"[MASK]\", \"[UNK]\"],\n",
    "        **kwargs,\n",
    "    )\n",
    "    return vocab\n",
    "\n",
    "def create_tokenizer(vocab, **kwargs):\n",
    "    tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
    "        vocabulary=vocab,\n",
    "        lowercase=True,\n",
    "        strip_accents=True,\n",
    "        oov_token=\"[UNK]\",\n",
    "        **kwargs,\n",
    "    )\n",
    "    return tokenizer\n",
    "\n",
    "def create_packer(tokenizer, **kwargs):\n",
    "    packer = keras_nlp.layers.StartEndPacker(\n",
    "        start_value=tokenizer.token_to_id(\"[START]\"),\n",
    "        end_value=tokenizer.token_to_id(\"[END]\"),\n",
    "        pad_value=tokenizer.token_to_id(\"[PAD]\"),\n",
    "        sequence_length=512,\n",
    "        **kwargs,\n",
    "    )\n",
    "    return packer\n",
    "\n",
    "def preprocess(x, y):\n",
    "    token_ids = packer(tokenizer(x))\n",
    "    return token_ids, y\n",
    "\n",
    "def get_prepr_dataset(dataset):\n",
    "    return dataset.map(preprocess, num_parallel_calls=AUTOTUNE).prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba71f77c-b9a0-4e3a-8ba6-252c5e1640e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(512,), dtype=int32, numpy=\n",
      "array([    1,  1437,   406,    16,  2578,   406,    25,   100,    96,\n",
      "         147,   328,    98,    96,   294,    16,    97,   130,   267,\n",
      "          24,    16,   102,    11,    61,  5943,    99,    96,   175,\n",
      "         385,   646,   125,   518,    97,    96,   283,   105,   102,\n",
      "          11,    61,    43,  1089,    17,  7418,   279,   601,    18,\n",
      "          32,   101,    19,    34,    32,   101,    19,    34,   646,\n",
      "          99,    96,  8873,    16,   131,    11,    61,   140,   690,\n",
      "         273,   263,   113,   219,    18,    18,    18,  4373,  5147,\n",
      "         395,    43,  2993, 10362,   868,  1055,  3114,  4702,    97,\n",
      "         118,  1037,   554,    11,    61,   494,   100, 15302,    18,\n",
      "        1824,    43,  3884,    16,   345,    35,   239,    16,    43,\n",
      "         208,   501,   868,  3314,   655,    43,  3496,   123,    96,\n",
      "        1402,   121,  2214,    16,    97,  3078,   102,    99,   261,\n",
      "          43,  1101,   113,    96, 17469,   108,   178,    18,   150,\n",
      "         240,   102,   397,     6,   181,    11,    62,  1012,  2518,\n",
      "        1089,     6,    16,   116,   881,    99,  1012,   102,   656,\n",
      "         111,   100,  2398,   125,   118,  1279,    16,   127,    61,\n",
      "        2739, 15529,   178,    97,  3636,   178,    99,  1532,    16,\n",
      "          97,  2176,    96,  3677,   405,    18,  1092,   100,    43,\n",
      "         208,   805,  1989,   105, 14167,  3174, 11598,  2052,  2936,\n",
      "          97,    43,   520,    16,    97,  5778,    99,   620,  1279,\n",
      "          18, 11446,    16,   379,   116,   234,   119,   408,   164,\n",
      "          17,   284,   680,    18,    98,   357,  3314,   100,   186,\n",
      "       10053,   125,    96,  4164,   327,   116,   392,   102,   129,\n",
      "          96,  6245,    16,   111,   116,   241,    11,    62,  2405,\n",
      "         148,    99,   120,   140,   546,  2052,    16,   116,   134,\n",
      "        3025,   755,    18,    32,   101,    19,    34,    32,   101,\n",
      "          19,    34,   131,    11,    61,    43,  1527,  3226,  7769,\n",
      "         280,    16,   127,   281,   147,  1038,   103,    96,  4210,\n",
      "         105,  1055,  3114,  4702,   259,    18,   161,   150,   159,\n",
      "         178,  3002,    43,   848,   145,  3314,    11,    61,  1648,\n",
      "         396,   178,    99,    96,  1236,    99,   261,    43,  3677,\n",
      "         108,   178,    99,   833,   178,   136,    98,   118,  7671,\n",
      "          18,    97,   139,   723,   100,   104,   318,   487,    35,\n",
      "         164,    16,   161,    11,   247,   115,   342,   111,   116,\n",
      "         216,   401,    99,   120,   760,   224,  4210,  1067,    99,\n",
      "         159,   139,   259,   188, 13675,    18,   116,   216,   482,\n",
      "         118, 14012,   129,    47,  8954,  1490,   178,   125,  2560,\n",
      "         178,    99,  1097,   178,   103,  2373,    96,   462,   340,\n",
      "          97,  2621,   178,   110,    43,     6,  2924,    96,  1175,\n",
      "         515,  4996,   374,     6,  2993,   108,   118,   626,    16,\n",
      "         111,    98,   357,     6,  2924,     6,   100,   115,    43,\n",
      "         142,  2993,    97,   306,   136,    98,    96,  1085,   103,\n",
      "          96,   617,    97,    98,   357,    16,   164,    16,   273,\n",
      "         802,    11,    62,   278,    18,    32,   101,    19,    34,\n",
      "          32,   101,    19,    34,   656,    16,   950,   139,    11,\n",
      "          61,   263,   113,   110,  1055,  3114,  4702,    97, 15302,\n",
      "         100,    98,   357,  2197,    16,    97,   107,   110,    96,\n",
      "         242,   155,    16, 15302,   100,   115,    43,     6,   237,\n",
      "         501,     6,    18, 15302,   100,   332,   150,   144,    43,\n",
      "       17140,    97,  5698,   179,   116,  2583,   129,     6,  3679,\n",
      "        7191, 10090,     6,    12,    43,  3775,  3319, 11687,    13,\n",
      "         128,   105,   189,  2779,   108,   118,   542,   875,    18,\n",
      "          97,    96,  2346,    98,    96,  7769,  3226,   100,  2197,\n",
      "         190,    16,    97,   131,    11,    61,   150,   334,    98,\n",
      "          43,   737,   368,    98,  2908,    18,    65,  5545,   374,\n",
      "          18,    32,   101,    19,    34,    32,   101,     2],\n",
      "      dtype=int32)>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n"
     ]
    }
   ],
   "source": [
    "vocabulary = create_vocab(vocab_size=30_000, dataset=train_ds)\n",
    "tokenizer = create_tokenizer(vocab=vocabulary)\n",
    "packer = create_packer(tokenizer)\n",
    "\n",
    "train_prepr = get_prepr_dataset(train_ds)\n",
    "val_prepr = get_prepr_dataset(val_ds)\n",
    "\n",
    "print(train_prepr.unbatch().take(1).get_single_element())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808ef9c6-434c-40b1-8dbc-126870fe4b1b",
   "metadata": {},
   "source": [
    "## Design a tiny transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ac6acea-f557-48f4-a8a7-b52031fc8bd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_9\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_9\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ token_and_position_embedding_4  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,342,912</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionEmbedding</span>)     │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,472</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ get_item_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ token_and_position_embedding_4  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │     \u001b[38;5;34m1,342,912\u001b[0m │\n",
       "│ (\u001b[38;5;33mTokenAndPositionEmbedding\u001b[0m)     │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │        \u001b[38;5;34m33,472\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ get_item_4 (\u001b[38;5;33mGetItem\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m130\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,376,514</span> (5.25 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,376,514\u001b[0m (5.25 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,376,514</span> (5.25 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,376,514\u001b[0m (5.25 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_model(vocab):\n",
    "    # model inputs \n",
    "    token_id_input = keras.Input(shape=(None,), dtype=\"int32\", name=\"token_ids\",)\n",
    "    # positional encoding + token encoding\n",
    "    outputs = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "        vocabulary_size=len(vocab),\n",
    "        sequence_length=packer.sequence_length,\n",
    "        embedding_dim=64)(token_id_input)\n",
    "    # 2-headed transformer encoder\n",
    "    outputs = keras_nlp.layers.TransformerEncoder(\n",
    "        num_heads=2,\n",
    "        intermediate_dim=128,\n",
    "        dropout=0.1)(outputs)\n",
    "    # the \"[START]\" token (id = 0) is used for classification\n",
    "    outputs = keras.layers.Dense(2)(outputs[:, 0, :])\n",
    "    model = keras.Model(inputs=token_id_input, outputs=outputs,)\n",
    "    return model\n",
    "\n",
    "def compile_model(model, lr_rate):\n",
    "    model.compile(\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer=keras.optimizers.AdamW(lr_rate),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "        jit_compile=True)\n",
    "\n",
    "def get_performance(model, dataset, metric=\"sparse_categorical_accuracy\"):\n",
    "    \"\"\"Evaluates model on a given dataset and returns specified metric value\"\"\"\n",
    "    return model.evaluate(dataset, return_dict=True, verbose=0)[metric]\n",
    "\n",
    "\n",
    "model = create_model(vocabulary)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647467bd-5da1-447a-a813-840ee729d2db",
   "metadata": {},
   "source": [
    "## Train the transformer directly on the classification objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efe69b0f-3ef0-4114-be5b-5bfbd51e93c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TriangularScheduler(keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"Linear ramp up for 'warmup' steps, then linear decay to 0 at 'total steps'.\"\"\"\n",
    "    def __init__(self, rate, warmup, total):\n",
    "        self.rate = rate\n",
    "        self.warmup = warmup\n",
    "        self.total = total\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\"rate\": self.rate, \"warmup\": self.warmup, \"total\": self.total}\n",
    "        return config\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = keras.ops.cast(step, dtype=\"float32\")\n",
    "        rate = keras.ops.cast(self.rate, dtype=\"float32\")\n",
    "        warmup = keras.ops.cast(self.warmup, dtype=\"float32\")\n",
    "        total = keras.ops.cast(self.total, dtype=\"float32\")\n",
    "\n",
    "        warmup_rate = rate * step / self.warmup\n",
    "        cooldown_rate = rate * (total - step) / (total - warmup)\n",
    "        triangular_rate = keras.ops.minimum(warmup_rate, cooldown_rate)\n",
    "        return keras.ops.maximum(triangular_rate, 0.0)\n",
    "\n",
    "epochs = 15\n",
    "batch_size = 32\n",
    "steps_per_epoch = train_ds.cardinality().numpy()\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "warmup_steps = int(0.1 * num_train_steps)\n",
    "initial_learning_rate = 5e-5\n",
    "\n",
    "warmup_schedule = TriangularScheduler(initial_learning_rate, warmup_steps, num_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "72a831d5-92e5-4ac4-9a82-d457fd5d6bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 17ms/step - loss: 0.7141 - sparse_categorical_accuracy: 0.4912 - val_loss: 0.6889 - val_sparse_categorical_accuracy: 0.5076\n",
      "Epoch 2/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 0.6874 - sparse_categorical_accuracy: 0.5404 - val_loss: 0.5697 - val_sparse_categorical_accuracy: 0.7522\n",
      "Epoch 3/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 0.5183 - sparse_categorical_accuracy: 0.7478 - val_loss: 0.4467 - val_sparse_categorical_accuracy: 0.7786\n",
      "Epoch 4/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 0.3627 - sparse_categorical_accuracy: 0.8421 - val_loss: 0.3343 - val_sparse_categorical_accuracy: 0.8538\n",
      "Epoch 5/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 0.2869 - sparse_categorical_accuracy: 0.8826 - val_loss: 0.3115 - val_sparse_categorical_accuracy: 0.8676\n",
      "Epoch 6/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 0.2564 - sparse_categorical_accuracy: 0.8977 - val_loss: 0.2877 - val_sparse_categorical_accuracy: 0.8846\n",
      "Epoch 7/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 0.2317 - sparse_categorical_accuracy: 0.9100 - val_loss: 0.2842 - val_sparse_categorical_accuracy: 0.8840\n",
      "Epoch 8/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 0.2143 - sparse_categorical_accuracy: 0.9184 - val_loss: 0.2746 - val_sparse_categorical_accuracy: 0.8886\n",
      "Epoch 9/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 0.2015 - sparse_categorical_accuracy: 0.9221 - val_loss: 0.2749 - val_sparse_categorical_accuracy: 0.8872\n",
      "Epoch 10/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 0.1768 - sparse_categorical_accuracy: 0.9335 - val_loss: 0.2794 - val_sparse_categorical_accuracy: 0.8894\n",
      "Epoch 11/15\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 0.1682 - sparse_categorical_accuracy: 0.9383 - val_loss: 0.2768 - val_sparse_categorical_accuracy: 0.8886\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f67a0259fd0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compile_model(model, lr_rate=warmup_schedule)\n",
    "\n",
    "e_stop = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
    "model.fit(\n",
    "    train_prepr, validation_data=val_prepr, epochs=15,\n",
    "    callbacks=[e_stop],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3e1432cc-c24f-4737-9e86-a7dc716a4421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.3019 - sparse_categorical_accuracy: 0.8815\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.31072551012039185, 0.8772000074386597]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(imdb_test_prepr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10158b6b-4e00-4255-9187-3edd3ec86658",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64e5a55-d3c4-46df-a7b8-4cefad1d18b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c29b97c-dae6-4512-a298-c9f2401f9e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a range of vocab sizes to explore\n",
    "vocab_sizes = [10_000, 30_000, 50_000, 70_000, 100_000]\n",
    "\n",
    "# dictionary to save models performances\n",
    "performance = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d3e9b68-3a33-4c83-bc3b-6c7c30eff112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - loss: 0.7273 - sparse_categorical_accuracy: 0.5052 - val_loss: 0.6898 - val_sparse_categorical_accuracy: 0.5076\n",
      "10000-vocabulary model performance on the validation set: 0.5076000094413757\n"
     ]
    }
   ],
   "source": [
    "# 10_000\n",
    "# data tokenization & packing\n",
    "vocabulary = create_vocab(vocab_size=vocab_sizes[0], dataset=train_ds)\n",
    "tokenizer = create_tokenizer(vocab=vocabulary)\n",
    "packer = create_packer(tokenizer)\n",
    "# get preprocessed training and validation data using new vocabulary \n",
    "train_prepr = get_prepr_dataset(train_ds)\n",
    "val_prepr = get_prepr_dataset(val_ds)\n",
    "# set up model and compile it\n",
    "model = create_model(vocabulary)\n",
    "compile_model(model, lr_rate=warmup_schedule)\n",
    "# train the model using triangular scheduling\n",
    "e_stop = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
    "model.fit(\n",
    "    train_prepr, validation_data=val_prepr, epochs=1, callbacks=[e_stop],\n",
    ")\n",
    "# get performance on training/validation sets to compare later \n",
    "performance[vocab_sizes[0]] = (get_performance(model, train_prepr), get_performance(model, val_prepr))\n",
    "print(f\"{vocab_sizes[0]}-vocabulary model performance on the validation set: {performance[vocab_sizes[0]][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d078b0-97b7-4dae-bb0f-dc033ede270d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2550259-0086-49e6-889c-8eaad266bcaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
